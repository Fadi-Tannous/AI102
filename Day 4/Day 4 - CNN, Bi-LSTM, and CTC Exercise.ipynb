{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - CNN, Bi-LSTM and CTC ##\n",
    "### Objetive: Exercise on CNN + Bi-LSTM + CTC Loss for Arabic Text Classification ###\n",
    "### Dataset: Labeled Handwritten Arabic Words ###\n",
    "### Please fill in all sections that start with \"# Task\" , sections that start with \"# Step\" are  pre-implemented #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1 - Dependencies & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1. - Install required libraries\n",
    "!pip3 install numpy matplotlib tensorflow keras scikit-learn --upgrade\n",
    "\n",
    "# Step 1.2. - Restart Kernel Manually\n",
    "# Toolbar -> Kernel -> Restart & Clear Output -> Restart & Clear All Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhzI1ZLchcd8"
   },
   "outputs": [],
   "source": [
    "# Step 1.3. - Import required libraries\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Input, Dropout, LSTM, Bidirectional, Reshape, StringLookup, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt, numpy as np, glob, pandas as pd, tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2 - Define Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1. Define Image Width & Height (All images will be resized accordingly)\n",
    "img_width = 64\n",
    "img_height = 32\n",
    "\n",
    "# Step 2.2. Define Padding Token (Integer) - Used to pad vectorized labels\n",
    "padding_token = 99\n",
    "\n",
    "# Step 2.3. Define Maximum Length of Label/Output (Longest Arabic word is 16 letters)\n",
    "max_label_length = 16\n",
    "\n",
    "# Step 2.4. Define Test Size for Train Test Split\n",
    "test_size = 0.33\n",
    "\n",
    "# Step 2.5. Define Number of Epochs/Iterations to Train the Model\n",
    "epochs = 10\n",
    "\n",
    "# Step 2.6 Define Batch Size for Model Training\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3 - Load Image Paths & Label Data ; and perform train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6k5QaeNxhqT_",
    "outputId": "68cf2c2c-e167-4233-bd3e-7f890b803486"
   },
   "outputs": [],
   "source": [
    "# Step 3.1. Load Images Paths and Label Strings into Pandas DataFrame\n",
    "dataset_path = \"Labeled_Handwritten_Arabic_Words/\"\n",
    "id_from_path = lambda x : x.split(\"\\\\\")[1].split(\".\")[0]\n",
    "images = [{\"id\": id_from_path(path), \"img_path\": path} for path in glob.glob(f\"{dataset_path}/images/*.jpg\")]\n",
    "labels = [{\"id\": id_from_path(path), \"label_str\": open(path, encoding=\"utf8\").read().strip()} for path in glob.glob(f\"{dataset_path}/labels/*.txt\")]\n",
    "df = pd.merge(pd.DataFrame(images), pd.DataFrame(labels), on='id')\n",
    "\n",
    "# Step 3.2. Train / Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['img_path'].tolist(), df['label_str'].tolist(), test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4 - Label Encoding & Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1. - Label Vectorization Class\n",
    "class label_vectorization(object):\n",
    "    \n",
    "    # Step 4.1.1. Define Initialization Function\n",
    "    def __init__(self, labels_list):\n",
    "        # Step 4.1.1.1. Extract a Sorted list of all unique characters within all labels\n",
    "        characters = sorted(list(set(char for label in labels_list for char in label)))\n",
    "        # Task 4.1.1.2. Create a String to Number Convertor using StringLookup\n",
    "\n",
    "        # Task 4.1.1.3. Create a Number to String Convertor using StringLookup (Inverse of 3.1.1.2)\n",
    "\n",
    "\n",
    "    # Step 4.1.2. Define vectorize function; which converts a string into a vector\n",
    "    def vectorize(self, label_str):\n",
    "        # Task 4.1.2.1. Use String to Number convertor for each character within the label_str\n",
    "\n",
    "        # Task 4.1.2.2. Pad the label_vector creating in 3.1.2.1 with padding_tokens; based on max_length - len(label_vector)\n",
    "\n",
    "        # Task 4.1.2.3. Return the Padded Label Vector\n",
    "\n",
    "\n",
    "    # Step 4.1.3. Define unvectorize function; which converts a vector into a string\n",
    "    def unvectorize(self, label_vector, pred=False):\n",
    "        # Step 4.1.3.1. Define which padding token to remove; if prediction then padding_token is -1\n",
    "        pad_token = -1 if pred else padding_token\n",
    "        # Task 4.1.3.2. Remove All Numbers within the label_vector which are equal to the padding token\n",
    "\n",
    "        # Task 4.1.3.3. Use the Number to String convertor to convert the label_vector to a string; hint(also use reduce_join)\n",
    "\n",
    "        # Task 4.1.3.4. Return the inverse of the string (as arabic is RTL)\n",
    "\n",
    "        \n",
    "# Step 4.2. Instantiate the Label Vectorizer\n",
    "label_vectorizer = label_vectorization(df['label_str'].tolist())\n",
    "\n",
    "# Step 4.3. Define Function that loads the image from path, preprocesses it and vectorizes the label\n",
    "#           Returning a dictionary with format {'image': image, 'label': vectorized_label}\n",
    "def preprocess_sample(im_path, label):\n",
    "    # Task 4.3.1. Read image using tf.io.read_file\n",
    "\n",
    "    # Task 4.3.2. Decode and convert to grayscale\n",
    "\n",
    "    # Task 4.3.3. Convert to float32 in [0, 1] range\n",
    "\n",
    "    # Task 4.3.4. Resize to the desired size\n",
    "\n",
    "    # Task 4.3.5. Transpose the image because we want the time dimension to correspond to the width of the image.\n",
    "\n",
    "    # Task 4.3.6. Return loaded & Preprocessing image\n",
    "\n",
    "\n",
    "# Step 4.4. Load & preprocess the training and testing datasets; formating them as tensorflow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.map(preprocess_sample, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.map(preprocess_sample, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5 - Build the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1. Define Connectionist Temporal Classification Loss Layer\n",
    "class CTCLayer(Layer):\n",
    "    \n",
    "    # Step 5.1.1. Define Initialization Function\n",
    "    def __init__(self, name=None):\n",
    "        # Trigger the generic \"Layer\" Initialization\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    # Step 5.1.2. Define the call function (which calculates the output and loss)\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Calculate the batch size\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        # Calculate the Input and Label Length (y_pred is the input and y_true is the label)\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        # Compute the ctc loss value\n",
    "        loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "        # Add Computed loss to the layer using `self.add_loss()`.\n",
    "        self.add_loss(loss)\n",
    "        # Return the Same outputs as received from the previous layer\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2. Define Model Architecture\n",
    "\n",
    "# Step 5.2.1. Define Input Layer with shape=(image width, image height, 1) and name = image\n",
    "input_img = Input(shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\")\n",
    "\n",
    "# Step 5.2.2. Define Input Layer with shape=(None,) and name = label\n",
    "labels = Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "\n",
    "# Task 5.2.3. Define 1st Convolutional Block (Conv2D, and MaxPooling2D)\n",
    "# Recommended Parameters: activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", pool size = 2, stride = 2\n",
    "\n",
    "\n",
    "# Task 5.2.4. Define 2nd Convolutional Block (Conv2D, and MaxPooling2D)\n",
    "# Recommended Parameters: activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\", pool size = 2, stride = 2\n",
    "# Size of 2nd Convolutional Block is usually larger\n",
    "\n",
    "\n",
    "# Step 5.2.5. We have used two max pool with pool size and strides 2. Hence, downsampled feature maps are 4x smaller.\n",
    "#The number of filters in the last layer is 64. Reshape accordingly before passing the output to the LSTM part of the model\n",
    "new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "x = Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "x = Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Task 5.2.6. Define 2x Bi-Directional LSTMS \n",
    "# Recommended Parameters: return_sequences=True, dropout=0.25\n",
    "\n",
    "\n",
    "# Step 5.2.7. Define Output layer; where number of outputs depends on the size of character vocabulary\n",
    "x = Dense(len(label_vectorizer.char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "# Step 5.2.8. Add CTC layer for calculating CTC loss at each step\n",
    "output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "# Step 5.2.9. Define overall model\n",
    "model = tf.keras.models.Model(inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\")\n",
    "\n",
    "# Step 5.2.10. Compile the model using Adam optimizer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Step 5.2.11. Train the model and use the testing data for validation\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 6 - Predict & Visualize Predictions Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1hPZfai5M3q",
    "outputId": "22f90cb9-b797-4cc3-ed8f-feb1e4b52e1a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 6.1. Get the prediction model by extracting layers till the output layer\n",
    "prediction_model = tf.keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "\n",
    "# Step 6.2.  Visualizethe Predictions\n",
    "# For 1 batch of the testing set\n",
    "for batch in test_dataset.take(1):\n",
    "    # Calculate the predictions by only supplying the image data\n",
    "    preds = prediction_model.predict(batch[\"image\"])    \n",
    "    # Decode the results of the CTC Layer\n",
    "    input_len = np.ones(preds.shape[0]) * preds.shape[1]\n",
    "    results = tf.keras.backend.ctc_decode(preds, input_length=input_len)[0][0][:, :max_label_length]\n",
    "    # Unvectorize the Decoded Predictions\n",
    "    pred_texts = [label_vectorizer.unvectorize(result, pred=True) for result in results]\n",
    "    \n",
    "    # For each image/prediction within the batch; add to subplot\n",
    "    _, ax = plt.subplots(4, 4, figsize=(15, 10))\n",
    "    for i in range(len(preds)):\n",
    "        img = (batch[\"image\"][i, :, :, 0] * 255).numpy().astype(np.uint8).T\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(f\"Prediction: {pred_texts[i]}\")\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Section 7 - Calculate Character Error Rate for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.1. Calculate the Character Error Rate for the test_dataset"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OCR_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
