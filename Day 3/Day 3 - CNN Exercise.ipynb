{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - CNNs ##\n",
    "### Objetive: Exercise on CNN for Arabic Text Classification ###\n",
    "### Dataset: Labeled Handwritten Arabic Words ###\n",
    "### Please fill in all sections that start with \"# Task\" , sections that start with \"# Step\" are  pre-implemented #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1 - Dependencies & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.1. - Install required libraries\n",
    "!pip3 install numpy swtloc matplotlib deskew opencv-python opencv-contrib-python tensorflow keras scikit-learn\n",
    "\n",
    "# Step 1.2. - Restart Kernel Manually\n",
    "# Toolbar -> Kernel -> Restart & Clear Output -> Restart & Clear All Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhzI1ZLchcd8"
   },
   "outputs": [],
   "source": [
    "# Step 1.3. - Import required libraries\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPool2D,InputLayer,Dropout,BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt, numpy as np, glob, cv2, pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2 - Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6k5QaeNxhqT_",
    "outputId": "68cf2c2c-e167-4233-bd3e-7f890b803486"
   },
   "outputs": [],
   "source": [
    "# Step 2.1. Load Images and Label Files into Pandas DataFrame\n",
    "dataset_path = \"Labeled_Handwritten_Arabic_Words/\"\n",
    "id_from_path = lambda x : x.split(\"\\\\\")[1].split(\".\")[0]\n",
    "images = [{\"id\": id_from_path(path), \"img\": cv2.resize(cv2.imread(path), (128, 64), interpolation=cv2.INTER_LINEAR)} for path in glob.glob(f\"{dataset_path}/images/*.jpg\")]\n",
    "labels = [{\"id\": id_from_path(path), \"label\": open(path, encoding=\"utf8\").read().strip()} for path in glob.glob(f\"{dataset_path}/labels/*.txt\")]\n",
    "df = pd.merge(pd.DataFrame(images), pd.DataFrame(labels), on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3 - Label Encoding & & Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1. - Encoding Classes; one-hot-encoding\n",
    "\n",
    "# Task 3.2. Train / Test Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4 - Build the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1hPZfai5M3q",
    "outputId": "22f90cb9-b797-4cc3-ed8f-feb1e4b52e1a"
   },
   "outputs": [],
   "source": [
    "# Task 4.1. - Extract the image dimension information\n",
    "\n",
    "\n",
    "# Task 4.2. - Initialize the sequential model and add an Input Layer that matches the image dimensions\n",
    "\n",
    "\n",
    "# Task 4.3. - Define a function that adds N convolution layers (conv+max pool) to the defined sequential model \n",
    "# The first layer starts with 15 filters (depth) then incrases by X with every extra layer.\n",
    "\n",
    "# N: Number of convolution layers to add \n",
    "# K: Kernel size for convolution layers\n",
    "# P: Pooling window size for the pooling layers\n",
    "# S: Strides for convolution and pooling layers\n",
    "# X: Number of filters to add with every new layer\n",
    "\n",
    "# Note: Use padding=\"same\" and activation=\"relu\" in the convolution layers\n",
    "\n",
    "\n",
    "# Task 4.4. - Call the defined function from Task 4.3 ; with N=3, K=3, P=2, S=2 and X=10\n",
    "\n",
    "\n",
    "# Task 4.5. - Define a function that adds a flattening layer followed by M fully connected layers and a dropout layer after each, \n",
    "# and finally a dense layer for the output classification\n",
    "\n",
    "# M: Number of fully connected (dense) layers\n",
    "# N: Number of neurons in each fully connected (dense) layer \n",
    "# prob: Dropout rate for dropout layer \n",
    "# n_classes: Number of output classes \n",
    "\n",
    "# Note: Use activation=\"relu\" in the dense layers except for the last layer where you should use activation='softmax' \n",
    "\n",
    "    \n",
    "# Task 4.6. - Call the defined function from Task 4.5 ; with M=2, N=256, Prob=0.2 and n_classes=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5 - Compile & Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task 5.1. -  Compile the model using Adam optimizer, categorical crossentropy loss function, and accuracy for the metric\n",
    "\n",
    "\n",
    "# Task 5.2. - Fit to training images and labels by setting the epochs to 5 and validation_split to 10%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 6 - Predict & Visualize Predictions Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6.1. - Predict for the testing set\n",
    "\n",
    "\n",
    "# Task 6.2 -  Visualize Predictions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OCR_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
